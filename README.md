# ğŸ¤– GPT2_LoRA_Jittor

æœ¬é¡¹ç›®åŸºäº [Jittor](https://github.com/Jittor/jittor) æ¡†æ¶ï¼Œåœ¨ç»å…¸è‹±æ–‡é—®ç­”æ•°æ®é›† [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) ä¸Šå¯¹ [GPT2](https://github.com/openai/gpt-2) è¿›è¡Œ LoRA å¾®è°ƒï¼Œåœ¨éƒ¨åˆ†æ•°æ®é›†çš„æƒ…å†µä¸‹éªŒè¯å…¶ç†è§£èƒ½åŠ›å¤ç°å¹¶å®ç°äº† GPT2 æ¨¡å‹çš„ LoRAï¼ˆLow-Rank Adaptationï¼‰å¾®è°ƒæ–¹æ³•ï¼Œæ”¯æŒä¸­æ–‡ä¸è‹±æ–‡ä»»åŠ¡çš„è®­ç»ƒä¸æ¨ç†ã€‚

> ğŸ”¬ æœ¬é¡¹ç›®æ—¨åœ¨éªŒè¯ LoRA å¾®è°ƒæŠ€æœ¯åœ¨ Jittor æ¡†æ¶ä¸‹çš„å¯è¡Œæ€§ä¸é«˜æ•ˆæ€§ï¼Œæä¾›å®Œæ•´çš„è®­ç»ƒã€æ¨ç†å’Œå¯è§†åŒ–æ”¯æŒã€‚

---

## ğŸ“ é¡¹ç›®ç»“æ„

```
jittor_lora_gpt2/
â”œâ”€â”€ GPT2_ji.py                   # GPT2 æ¨¡å‹ç»“æ„ï¼ˆBlock, Attention, Feedforward ç­‰ï¼‰
â”œâ”€â”€ LoRA.py                      # LoRA æ’å…¥æ¨¡å—ï¼Œæ”¯æŒä½ç§©é€‚é…
â”œâ”€â”€ tokenizer_loader.py          # Tokenizer åŠ è½½æ¨¡å—ï¼ˆå…¼å®¹ Huggingfaceï¼‰
â”œâ”€â”€ dataset_loader.py            # æ•°æ®é›†é¢„å¤„ç†ä¸åŠ è½½ï¼ˆæ”¯æŒä¸­è‹±æ–‡ï¼‰
â”œâ”€â”€ GPT2_LoRA_Full_Experiment.py # ä¸»è®­ç»ƒè„šæœ¬ï¼ˆæ”¯æŒè®­ç»ƒä¸æµ‹è¯•ï¼‰
â”œâ”€â”€ plot_loss.py                 # ç»˜åˆ¶è®­ç»ƒè¿‡ç¨‹æŸå¤±æ›²çº¿
â”œâ”€â”€ config/                      # å¯é€‰çš„æ¨¡å‹é…ç½®æ–‡ä»¶ç›®å½•
â”œâ”€â”€ checkpoints/                 # ä¿å­˜æ¨¡å‹æƒé‡çš„ç›®å½•
â””â”€â”€ README.md
```

---

## ğŸ”§ ç¯å¢ƒä¾èµ–

- Python â‰¥ 3.8
- Jittor â‰¥ 1.3.7.0ï¼ˆå»ºè®® GPU ç‰ˆæœ¬ï¼‰
- Transformersï¼ˆç”¨äºåŠ è½½ GPT2 tokenizer å’Œé¢„è®­ç»ƒæƒé‡ï¼‰
- å…¶ä»–ä¾èµ–ï¼š`tqdm`, `numpy`, `matplotlib`

### å®‰è£…å‘½ä»¤

```bash
pip install jittor==1.3.7.0
pip install transformers==4.30.0
pip install matplotlib tqdm numpy
```

> ğŸ’¡ è¯·ç¡®ä¿ä½ æ­£ç¡®å®‰è£…äº† GPU ç‰ˆ Jittorï¼Œå‚è€ƒå®˜ç½‘å®‰è£…è¯´æ˜ï¼š[https://cg.cs.tsinghua.edu.cn/jittor/install](https://cg.cs.tsinghua.edu.cn/jittor/install)

---

## ğŸš€ å¿«é€Ÿå¼€å§‹

### 1ï¸âƒ£ å‡†å¤‡ GPT2 Tokenizer ä¸é¢„è®­ç»ƒæ¨¡å‹

```python
from transformers import GPT2Tokenizer, GPT2Model
GPT2Tokenizer.from_pretrained("gpt2").save_pretrained("./gpt2")
GPT2Model.from_pretrained("gpt2").save_pretrained("./gpt2")
```

### 2ï¸âƒ£ è¿è¡Œå¾®è°ƒè„šæœ¬

```bash
python GPT2_LoRA_Full_Experiment.py
```

### 3ï¸âƒ£ æŸ¥çœ‹ç”Ÿæˆæ–‡æœ¬

```
è¾“å…¥æç¤ºè¯ï¼šäººå·¥æ™ºèƒ½
ç”Ÿæˆç»“æœï¼šäººå·¥æ™ºèƒ½æ­£åœ¨æ”¹å˜äººç±»çš„ç”Ÿæ´»æ–¹å¼ï¼Œåœ¨åŒ»ç–—ã€æ•™è‚²å’Œäº¤é€šç­‰é¢†åŸŸå±•ç°å·¨å¤§æ½œåŠ›â€¦â€¦
```

---

## ğŸ§  LoRA æ¨¡å—æœºåˆ¶ç®€ä»‹

LoRA æ˜¯ä¸€ç§ä½ç§©çŸ©é˜µè¿‘ä¼¼å¾®è°ƒæ–¹æ³•ï¼Œå¯å‡å°‘å‚æ•°æ›´æ–°é‡å¹¶åŠ å¿«è®­ç»ƒé€Ÿåº¦ã€‚æœ¬é¡¹ç›®ä¸­ LoRA åº”ç”¨äº GPT2 çš„ Attention å­å±‚ã€‚

```python
class LoRALinear(nn.Module):
    def __init__(self, in_features, out_features, r=4, alpha=32):
        self.lora_A = nn.Linear(in_features, r, bias=False)
        self.lora_B = nn.Linear(r, out_features, bias=False)
        self.scaling = alpha / r
    def execute(self, x):
        return self.lora_B(self.lora_A(x)) * self.scaling
```

æ³¨å…¥æ–¹å¼ï¼š

```python
# æ›¿æ¢åŸå§‹ Q/K/V å±‚ä¸º LoRA æ³¨å…¥å±‚
self.q_proj = LoRAInjectedLinear(original_q_proj)
```

---

## ğŸ“Š å®éªŒç»“æœç¤ºä¾‹

| Epoch | Loss  | Perplexity |
|-------|-------|------------|
|   1   | 2.63  | 13.9       |
|   2   | 1.95  | 7.03       |
|   3   | 1.52  | 4.58       |

### ğŸ“ˆ æŸå¤±æ›²çº¿å¯è§†åŒ–

ä¿å­˜è®­ç»ƒæ—¶æŸå¤±çš„ loss.txt æ–‡ä»¶åè¿è¡Œï¼š

```bash
python plot_loss.py
```

è¾“å‡ºå›¾ç¤ºï¼š

![loss curve](./images/loss_curve.png)

---

## ğŸ“š å‚è€ƒèµ„æ–™

- ğŸ”– [LoRA: Low-Rank Adaptation of Large Language Models](https://arxiv.org/abs/2106.09685)
- ğŸ”– [Jittor: A Novel Deep Learning Framework](https://github.com/Jittor/jittor)
- ğŸ”– [Huggingface Transformers](https://huggingface.co/docs/transformers)

---

## ğŸ“ å¼•ç”¨æœ¬é¡¹ç›®

å¦‚æœä½ åœ¨å­¦æœ¯æˆ–å·¥ç¨‹é¡¹ç›®ä¸­ä½¿ç”¨äº†æœ¬é¡¹ç›®ï¼Œè¯·å¼•ç”¨ï¼š

```bibtex
@misc{jittor-lora-gpt2,
  author = {Your Name},
  title = {Jittor Implementation of LoRA on GPT2},
  year = {2025},
  howpublished = {\url{https://github.com/yourname/jittor-lora-gpt2}}
}
```

---

## ğŸ¤ è´¡çŒ®æ–¹å¼

æ¬¢è¿è´¡çŒ®ä»£ç ã€æ”¹è¿›æ–‡æ¡£æˆ–æŠ¥å‘Šé—®é¢˜ï¼š

```bash
# fork ä»“åº“åæäº¤ PR
git clone https://github.com/yourname/jittor-lora-gpt2.git
```

æ¬¢è¿ star â­ æœ¬é¡¹ç›®ä»¥ç¤ºé¼“åŠ±ï¼
```
