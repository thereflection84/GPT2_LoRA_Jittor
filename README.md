# GPT2_LoRA_Jittor

æœ¬é¡¹ç›®åŸºäº [Jittor](https://github.com/Jittor/jittor) æ¡†æ¶ï¼Œåœ¨ç»å…¸è‹±æ–‡é—®ç­”æ•°æ®é›† [SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) ä¸Šå¯¹ [GPT2](https://github.com/openai/gpt-2) è¿›è¡Œ LoRA å¾®è°ƒï¼Œåœ¨éƒ¨åˆ†æ•°æ®é›†çš„æƒ…å†µä¸‹éªŒè¯å…¶ç†è§£èƒ½åŠ›å¤ç°å¹¶å®ç°äº† GPT2 æ¨¡å‹çš„ LoRAï¼ˆLow-Rank Adaptationï¼‰å¾®è°ƒæ–¹æ³•ï¼Œæ”¯æŒè‹±æ–‡ä»»åŠ¡çš„è®­ç»ƒä¸æ¨ç†ã€‚

>  æœ¬é¡¹ç›®æ—¨åœ¨éªŒè¯ LoRA å¾®è°ƒæŠ€æœ¯åœ¨ Jittor æ¡†æ¶ä¸‹çš„å¯è¡Œæ€§ä¸é«˜æ•ˆæ€§ï¼Œæä¾›å®Œæ•´çš„è®­ç»ƒã€æ¨ç†å’Œå¯è§†åŒ–æ”¯æŒã€‚

---

## é¡¹ç›®ç»“æ„

```
GPT2_LoRA_Jittor/
â”œâ”€â”€ GPT2_jittor.py               # GPT2 æ¨¡å‹ç»“æ„ï¼ˆBlock, Attention, Feedforward ç­‰ï¼‰
â”œâ”€â”€ LoRA.py                      # åŒ…å«LoRAæ ¸å¿ƒå®ç°
â”œâ”€â”€ lora_models.py               # åŒ…å«LoRAåº”ç”¨äºGPT2çš„æ¨¡å‹å®ç°
â”œâ”€â”€ gpt2_lora_dataset.py         # æ•°æ®é›†é¢„å¤„ç†ä¸åŠ è½½
â”œâ”€â”€ train_default_lora.py        # ä¸»è®­ç»ƒè„šæœ¬ï¼ˆæ”¯æŒè®­ç»ƒä¸æµ‹è¯•ï¼‰
â”œâ”€â”€ test_generation.py           # ä¸»æµ‹è¯•è„šæœ¬
â”œâ”€â”€ plot_loss.py                 # ç»˜åˆ¶è®­ç»ƒè¿‡ç¨‹æŸå¤±æ›²çº¿
â”œâ”€â”€ model_utils.py               # å›ç­”ç”Ÿæˆæ•°æ®åŠ è½½ä¸è¯„ä¼°æ¨¡å—
â”œâ”€â”€ checkpoints/                 # è®­ç»ƒæ—¥å¿—ç›®å½•
â””â”€â”€ README.md
```

---

## ç¯å¢ƒä¾èµ–

- Python â‰¥ 3.8
- Jittor â‰¥ 1.3.7.0ï¼ˆå»ºè®® GPU ç‰ˆæœ¬ï¼‰
- Transformersï¼ˆç”¨äºåŠ è½½ GPT2 tokenizer å’Œé¢„è®­ç»ƒæƒé‡ï¼‰
- å…¶ä»–ä¾èµ–ï¼š`tqdm`, `numpy`, `matplotlib`
- å…·ä½“å‚è€ƒ requirement.txt

### å®‰è£…å‘½ä»¤

```bash
conda create -n env python=3.9
conda activate env  # å¿…é¡»æ¿€æ´»ç¯å¢ƒï¼
conda install jittor=1.3.7.0 transformers=4.53.0 matplotlib tqdm numpy -c conda-forge
```


---

## å¿«é€Ÿå¼€å§‹

### å‡†å¤‡ GPT2 Tokenizer ä¸é¢„è®­ç»ƒæ¨¡å‹

```python
from transformers import GPT2Tokenizer, GPT2Model
GPT2Tokenizer.from_pretrained("gpt2").save_pretrained("./gpt2")
GPT2Model.from_pretrained("gpt2").save_pretrained("./gpt2")
```

### è¿è¡Œå¾®è°ƒè„šæœ¬

```bash
python train_default_lora.py
```

### æŸ¥çœ‹ç”Ÿæˆæ–‡æœ¬

```
Context: Low-Rank Adaptation (LoRA) is a parameter-efficient fine-tuning technique that works by freezing the pre-trained model weights and injecting trainable low-rank matrices into each layer of the Transformer architecture, greatly reducing the number of trainable parameters for downstream tasks.
Question: What is the main purpose of LoRA?
Answer: The main objective of the process is to reduce the number of repetitions required to create a realistic human model. This reduces the number of repetitions required to create a realistic model from 1,200 to 1,500 repetitions in the current study.
```

---

## LoRA æ¨¡å—æœºåˆ¶ç®€ä»‹

LoRA æ˜¯ä¸€ç§ä½ç§©çŸ©é˜µè¿‘ä¼¼å¾®è°ƒæ–¹æ³•ï¼Œå¯å‡å°‘å‚æ•°æ›´æ–°é‡å¹¶åŠ å¿«è®­ç»ƒé€Ÿåº¦ã€‚æœ¬é¡¹ç›®ä¸­ LoRA åº”ç”¨äº GPT2 çš„ Attention å­å±‚ä¸å‰é¦ˆç½‘ç»œå±‚ã€‚

```python
class LoRALinear(nn.Module):
    def __init__(self, in_features, out_features, r=8, lora_alpha=16, lora_dropout=0.1):
        super().__init__()

        # å†»ç»“åŸå§‹æƒé‡å’Œåç½®
        self.weight = jt.init.gauss((out_features, in_features), 'float32', std=0.02)
        self.weight.requires_grad = False

        self.bias = jt.zeros(out_features)
        self.bias.requires_grad = False

        # LoRA å­å±‚ï¼ˆä½ç§©åˆ†è§£ï¼‰
        self.lora_A = nn.Linear(in_features, r, bias=False)
        self.lora_B = nn.Linear(r, out_features, bias=False)

        # åˆå§‹åŒ–
        self.lora_A.weight = jt.init.gauss(self.lora_A.weight.shape, 'float32', std=1.0/r)
        self.lora_B.weight = jt.zeros(self.lora_B.weight.shape)

        self.scaling = lora_alpha / r
        self.lora_dropout = nn.Dropout(p=lora_dropout)

    def execute(self, x):
        # åŸå§‹æƒé‡è·¯å¾„ï¼ˆå†»ç»“ï¼‰
        out = jt.matmul(x, self.weight.t()) + self.bias
        # LoRA è·¯å¾„ï¼ˆå¯è®­ç»ƒï¼‰
        out += self.lora_B(self.lora_A(self.lora_dropout(x))) * self.scaling
        return out
```

æ³¨å…¥æ–¹å¼ï¼š

```python
# æ›¿æ¢åŸå§‹ å¤šå¤´æ³¨æ„åŠ›å±‚ ä¸º LoRA æ³¨å…¥å±‚
 self.attn = LoRACausalSelfAttention(
                config, 
                lora_config, 
                original_c_attn=original_c_attn,
                original_c_proj=original_c_proj
            )
```

---


### æŸå¤±æ›²çº¿å¯è§†åŒ–

ä¿å­˜è®­ç»ƒæ—¶æŸå¤±çš„ txt æ–‡ä»¶åè¿è¡Œï¼š

```bash
python plot_loss.py
```

è¾“å‡ºå›¾ç¤ºï¼ˆè¿™é‡Œç”±äºç¬”è®°æœ¬ç”µè„‘æ˜¾å­˜ä¸è¶³ï¼Œå› æ­¤ç”¨çš„åªæ˜¯éƒ¨åˆ†æ•°æ®é›†è®­ç»ƒï¼Œæ•ˆæœå¹¶ä¸ç®—å¾ˆå¥½ï¼‰ï¼š

![lora_training_curves](lora_training_curves.png)

---

## å‚è€ƒèµ„æ–™

- ğŸ”– [LoRA: Low-Rank Adaptation of Large Language Models](https://arxiv.org/abs/2106.09685)
- ğŸ”– [Jittor: A Novel Deep Learning Framework](https://github.com/Jittor/jittor)
- ğŸ”– [Huggingface Transformers](https://huggingface.co/docs/transformers)

---
